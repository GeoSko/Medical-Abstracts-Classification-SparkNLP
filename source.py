# -*- coding: utf-8 -*-
"""Spark-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jFOhXz-jCIob0Hw8mnE26YDlBqQ4k_NY

# Imports
"""

!pip install pyspark
!pip install sparknlp

!git clone https://github.com/sebischair/Medical-Abstracts-TC-Corpus.git

"""# Load Data"""

import sparknlp
spark = sparknlp.start(gpu=True)

# # path to data folder
train_path = "Medical-Abstracts-TC-Corpus/medical_tc_train.csv"
test_path = "Medical-Abstracts-TC-Corpus/medical_tc_test.csv"

# data load
df_train = spark.read.csv( train_path , header=True, inferSchema=True)
df_test = spark.read.csv( test_path , header=True, inferSchema=True)

df_train

df_train.show()

df_test.show()

"""# Remove stopwords"""

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

stop_words = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your",
            "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her",
            "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs",
            "themselves", "what", "which", "who", "whom", "this", "that", "these", "those",
            "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had",
            "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if",
            "or","because", "as", "until", "while", "of", "at", "by", "for", "with", "about",
            "against", "between", "into", "through", "during", "before", "after", "above",
            "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under",
            "again", "further", "then", "once", "here", "there", "when", "where", "why",
            "how", "all", "any", "both", "each", "few", "more", "most", "other", "some",
            "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very",
            "s", "t", "can", "will", "just", "don", "should", "now"]

def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

remove_stopwords_udf = udf(remove_stopwords, StringType())

df_train = df_train.withColumn("medical_abstract", remove_stopwords_udf("medical_abstract"))

df_train.show()

"""# Word Embeddings"""

# Import the required modules and classes
from sparknlp.base import DocumentAssembler, Pipeline
from sparknlp.annotator import Tokenizer,WordEmbeddingsModel

# Step 1: Transforms raw texts to `document` annotation
documentAssembler = DocumentAssembler() \
      .setInputCol('medical_abstract') \
      .setOutputCol('document')

# Step 2: Tokenization
tokenizer = Tokenizer() \
      .setInputCols(['document']) \
      .setOutputCol('token')

# Step 3: Generate the Embeddings
embeddings = WordEmbeddingsModel\
      .pretrained('glove_100d', 'en')\
      .setInputCols(["token", "document"])\
      .setOutputCol("embeddings")

# Define the pipeline
pipeline = Pipeline() \
    .setStages([
        documentAssembler,
        tokenizer,
        embeddings])


# Fit the dataframe to the pipeline and then transform to produce the embeddings
model = pipeline.fit(df_train)

result = model.transform(df_train)

result

result.select("embeddings").embeddings



# Import the required modules and classes
from sparknlp.base import DocumentAssembler, Pipeline, LightPipeline, EmbeddingsFinisher
from sparknlp.annotator import Tokenizer,WordEmbeddingsModel,SentenceEmbeddings
import pyspark.sql.functions as F


def my_word_embeddings(df, method):
  document_assembler = DocumentAssembler() \
        .setInputCol("medical_abstract") \
        .setOutputCol("document")

  tokenizer = Tokenizer() \
        .setInputCols(["document"]) \
        .setOutputCol("token")
  glove_embeddings = WordEmbeddingsModel().pretrained() \
        .setInputCols(["document",'token'])\
        .setOutputCol("embeddings")\
        .setCaseSensitive(True)
  sentence_embeddings = SentenceEmbeddings() \
        .setInputCols(["document", "embeddings"]) \
        .setOutputCol("sentence_embeddings") \
        .setPoolingStrategy(method)

  embeddings_finisher = EmbeddingsFinisher() \
        .setInputCols(["sentence_embeddings"]) \
        .setOutputCols(["finished_sentence_embeddings"])
  pipeline = Pipeline(
      stages=[document_assembler,
              tokenizer,
              glove_embeddings,
              sentence_embeddings,
              embeddings_finisher])

  model = pipeline.fit(df)
  result = model.transform(df)
  return result

WE_avg_train = my_word_embeddings(df_train, "AVERAGE")
WE_avg_test = my_word_embeddings(df_test, "AVERAGE")

WE_sum_train = my_word_embeddings(df_train, "SUM")
WE_sum_test = my_word_embeddings(df_test, "SUM")

from pyspark.sql.functions import col
from pyspark.sql.types import StringType, ArrayType, DoubleType

import pyspark.sql.functions as F
import pyspark.sql.types as T

# WE_avg_train = WE_avg_train.withColumn("condition_label",F.array(F.col("condition_label").cast(F.StringType())))
# WE_avg_test = WE_avg_test.withColumn("condition_label",F.array(F.col("condition_label").cast(F.StringType())))

# WE_sum_train = WE_sum_train.withColumn("condition_label",F.array(F.col("condition_label").cast(F.StringType())))
# WE_sum_test = WE_sum_test.withColumn("condition_label",F.array(F.col("condition_label").cast(F.StringType())))

WE_avg_train = WE_avg_train.withColumn("condition_label",F.col("condition_label").cast("double"))
WE_avg_test = WE_avg_test.withColumn("condition_label",F.col("condition_label").cast("double"))

WE_sum_train = WE_sum_train.withColumn("condition_label",F.col("condition_label").cast("double"))
WE_sum_test = WE_sum_test.withColumn("condition_label",F.col("condition_label").cast("double"))

WE_avg_train

WE_sum_train.selectExpr("explode(finished_sentence_embeddings) as finished_sentence_embeddings").show(truncate=False)

"""# Bert"""

from sparknlp.base import DocumentAssembler
from sparknlp.annotator import SentenceDetector, BertSentenceEmbeddings
from pyspark.ml import Pipeline
import pyspark.sql.functions as F
documentAssembler = DocumentAssembler() \
    .setInputCol("medical_abstract") \
    .setOutputCol("document")
sentence = SentenceDetector() \
    .setInputCols(["document"]) \
    .setOutputCol("sentence")
embeddings = BertSentenceEmbeddings.pretrained("sent_small_bert_L2_128") \
    .setInputCols(["sentence"]) \
    .setOutputCol("sentence_embeddings")\
    .setCaseSensitive(True) \
    .setMaxSentenceLength(512) \

pipeline = Pipeline(stages=[documentAssembler,
                            sentence,
                            embeddings])

model = pipeline.fit(df_train)
BERT_train = model.transform(df_train)
BERT_test = model.transform(df_test)

# BERT_train = BERT_train.withColumn("condition_label",F.array(F.col("condition_label").cast(F.StringType())))
# BERT_test = BERT_test.withColumn("condition_label",F.array(F.col("condition_label").cast(F.StringType())))

BERT_test

result_df = BERT_test.select(F.explode(F.arrays_zip
                        (BERT_test.sentence.result,
                         BERT_test.sentence_embeddings.embeddings)).alias("cols")) \
                  .select(F.expr("cols['0']").alias("sentence"),
                          F.expr("cols['1']").alias("Bert_sentence_embeddings"))
result_df.show(truncate=150)

"""# Models

## Neural Network
"""

from sklearn.metrics import classification_report

def evaluate(model, dataset,name, ev_name):
  dataset = model.transform(dataset)
  dataset = dataset.select('condition_label',"class.result").toPandas()
  dataset['result'] = dataset['result'].apply(lambda x : x[0])
  dataset['condition_label'] = dataset['condition_label'].astype(str)
  print(name)
  print(ev_name)
  print(classification_report(dataset['condition_label'], dataset['result']))



from sparknlp.annotator import ClassifierDLApproach


classsifierdl = ClassifierDLApproach()\
                  .setInputCols(["sentence_embeddings"])\
                  .setOutputCol("class")\
                  .setLabelColumn("condition_label")\
                  .setBatchSize(64)\
                  .setMaxEpochs(20)\
                  .setLr(0.005)\
                  .setDropout(0.5)

                  # .setLr(0.01) \
                  # .setBatchSize(64)\
                  # .setMaxEpochs(50)\

NN_WE_avg_model = classsifierdl.fit(WE_avg_train)

evaluate(NN_WE_avg_model, WE_avg_train,"Neural Nework Word Embeddings(AVG)", "Training Set")

evaluate(NN_WE_avg_model, WE_avg_test,"Neural Nework Word Embeddings(AVG)", "Test Set")

NN_WE_sum_model = classsifierdl.fit(WE_sum_train)

evaluate(NN_WE_sum_model, WE_sum_train,"Neural Nework Word Embeddings(SUM)", "Training Set")

evaluate(NN_WE_sum_model, WE_sum_test,"Neural Nework Word Embeddings(SUM)", "Test Set")

NN_BERT_model = classsifierdl.fit(BERT_train)

evaluate(NN_BERT_model, BERT_train,"Neural Nework BERT", "Training Set")

evaluate(NN_BERT_model, BERT_test,"Neural Nework BERT", "Test Set")

"""## SVM"""

from pyspark.ml.classification import LinearSVC, OneVsRest
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql import functions as F
from pyspark.sql.types import ArrayType

# Create a LinearSVC classifier
svm = LinearSVC(
    maxIter=50,
    regParam=0.1,
    labelCol="label",
    featuresCol="features"
)

from pyspark.ml.feature import StringIndexer
stringIndexer = StringIndexer(inputCol="condition_label", outputCol="label")



def df_prepare(df):
  # Flatten the nested array
  df_flattened = df.withColumn(
      "flattened_sentence_embeddings",
      F.flatten(F.col("finished_sentence_embeddings"))
  )

  df_flattened = stringIndexer.fit(df_flattened).transform(df_flattened)

  # Convert the flattened array into a vector
  to_vector_udf = F.udf(lambda a: Vectors.dense(a), VectorUDT())
  df_flattened = df_flattened.withColumn(
      "features",
      to_vector_udf(F.col("flattened_sentence_embeddings"))
  )
  return df_flattened



# Create a One-vs-Rest classifier
ovr = OneVsRest(
    classifier=svm,
)

# # Fit the One-vs-Rest classifier on the flattened DataFrame
# model = ovr.fit(df_prepare(we_avg_train))

# # Make predictions on the dataset
# predictions = model.transform(we_avg_train_flattened)

# # Evaluate the model using a multi-class classification evaluator
# evaluator = MulticlassClassificationEvaluator(
#     labelCol="condition_label",
#     metricName="accuracy"
# )
# accuracy = evaluator.evaluate(predictions)

# # Print the accuracy
# print("Accuracy:", accuracy)

def svm_evaluate(model, dataset,name, ev_name):
  dataset = model.transform(df_prepare(dataset))
  dataset = dataset.select('label',"Prediction").toPandas()
  # dataset['label'] = dataset['label'].astype(str)
  # dataset['Prediction'] = dataset['Prediction'].astype(str)
  print(name)
  print(ev_name)
  print(classification_report(dataset['label'], dataset['Prediction']))

SVM_WE_avg_model = ovr.fit(df_prepare(WE_avg_train))

svm_evaluate(SVM_WE_avg_model, WE_avg_train,"SVM Word Embeddings(AVG)", "Training Set")

svm_evaluate(SVM_WE_avg_model, WE_avg_test,"SVM Word Embeddings(AVG)", "Test Set")

SVM_WE_sum_model = ovr.fit(df_prepare(WE_sum_train))

svm_evaluate(SVM_WE_sum_model, WE_sum_train,"SVM Word Embeddings(SUM)", "Training Set")

svm_evaluate(SVM_WE_sum_model, WE_sum_test,"SVM Word Embeddings(SUM)", "Test Set")

BERT_train_SVM = BERT_train.withColumn(
      "flattened_sentence_embeddings",
      F.flatten(F.col("sentence_embeddings.embeddings"))
  )

BERT_train_SVM = BERT_train_SVM.select(
    "condition_label",
    "medical_abstract",
    "document",
    "sentence",
    "sentence_embeddings",
    "flattened_sentence_embeddings",
    slice("flattened_sentence_embeddings", 1, 100).alias("features")
)

BERT_train_SVM = stringIndexer.fit(BERT_train_SVM).transform(BERT_train_SVM)

  # Convert the flattened array into a vector
to_vector_udf = F.udf(lambda a: Vectors.dense(a), VectorUDT())
BERT_train_SVM = BERT_train_SVM.withColumn(
      "features",
      to_vector_udf(F.col("features"))
)

# BERT_train_SVM = BERT_train_SVM.withColumn("label",
#       F.col("condition_label"))

SVM_BERT_model = ovr.fit(BERT_train_SVM)

from sklearn.metrics import classification_report
def svm_evaluate(model, dataset,name, ev_name):
  dataset = model.transform(dataset)
  dataset = dataset.select('label',"Prediction").toPandas()
  # dataset['label'] = dataset['label'].astype(str)
  # dataset['Prediction'] = dataset['Prediction'].astype(str)
  print(name)
  print(ev_name)
  print(classification_report(dataset['label'], dataset['Prediction']))

svm_evaluate(SVM_BERT_model, BERT_train_SVM,"SVM BERT", "Training Set")

BERT_test_SVM = BERT_test.withColumn(
      "flattened_sentence_embeddings",
      F.flatten(F.col("sentence_embeddings.embeddings"))
  )

BERT_test_SVM = BERT_test_SVM.select(
    "condition_label",
    "medical_abstract",
    "document",
    "sentence",
    "sentence_embeddings",
    "flattened_sentence_embeddings",
    slice("flattened_sentence_embeddings", 1, 100).alias("features")
)

BERT_test_SVM = stringIndexer.fit(BERT_test_SVM).transform(BERT_test_SVM)

  # Convert the flattened array into a vector
to_vector_udf = F.udf(lambda a: Vectors.dense(a), VectorUDT())
BERT_test_SVM = BERT_test_SVM.withColumn(
      "features",
      to_vector_udf(F.col("features"))
)

svm_evaluate(SVM_BERT_model, BERT_test_SVM,"SVM BERT", "Test Set")